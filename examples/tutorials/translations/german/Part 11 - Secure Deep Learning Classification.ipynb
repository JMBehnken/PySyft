{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T09:15:10.188758Z",
     "start_time": "2020-04-04T09:15:10.185725Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "n_test_batches = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 11 - Sichere Deep Learning Klassifikation \n",
    "\n",
    "\n",
    "\n",
    "## Die Daten sind von Bedeutung, das Model ebenfalls\n",
    "\n",
    "Die Daten treiben das Machine Learning voran. Organisationen welche Daten erschaffen und sammeln sind in der Lage selbst Machine Learning Modele zu trainieren. Dies ermöglicht ihnen ihre Modele als Service (MLaaS) anderen Organisationen bereit zu stellen. Dies ist insbesondere nützlich für Organisationen, die nicht genug Daten für ein eigenes Model besitzen, aber dennoch von Vorhersagen solch eines Models profitieren möchten. \n",
    "\n",
    "Doch so ein bereit gestelltes Model in der Cloud stellt immer noch ein Problem für die Privatsphäre dar. Um von anderen Organisationen genutzt werden zu können, müssen entweder die Input-Daten (wie z. B. zu klassifizierende Bilder) hochgeladen oder das gesamte Model herunter geladen werden. Das Hochladen der Daten kann problematisch sein aus Sicht der zu schützenden Privatsphäre. Das Herunterladen des Models ist jedoch nicht immer eine Option, wenn zum Beispiel die Organisation dadurch ihr geistiges Eigentum verliert.\n",
    "\n",
    "## Berechnungen mit verschlüsselten Daten\n",
    "\n",
    "In diesem Kontext kann eine mögliche Lösung im Verschlüsseln der Daten und des Models liegen. So können nach dem Verschlüsseln verschiedene Organisationen zusammen arbeiten, ohne gleich ihr IP preiszugeben. Es existieren mehrere Verschlüsselungs Schemata, die es ermöglichen mit verschlüsselten Daten zu rechnen. Zu den Bekanntesten gehören \"Secure Multi-Party Computation\" (SMPC), \"Homomorphic Encryption\" (FHE/SHE) und \"Functional Encryption\" (FE). Hier wird der Fokus auf \"Secure Multi-Party Computation\" mit dem additiven Aufteilen aus [Tutorial 5](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%205%20-%20Intro%20to%20Encrypted%20Programs.ipynb) gelegt. Dabei wird auf den Crypto-Protokollen SecureNN und SPDZ ([Blogpost](https://mortendahl.github.io/2017/09/19/private-image-analysis-with-mpc/)) aufgebaut. \n",
    "\n",
    "Diese Protokolle zeigen außergewöhnliche Performance auf verschlüsselten Daten und in den vergangenen Monaten wurde daran gearbeitet sie einfacher nutzen zu können. Im Speziellen wurde an Werkzeugen gearbeitet, die eine Nutzung der Protokolle ermöglichen, ohne sie selbst implementieren zu müssen oder gar ein tieferes Verständnis für die zugrundeliegenden Strukturen zu besitzen.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Der genaue Hintergrund dieses Tutorials ist wie folgt: es gibt einen Server mit einigen Daten. Zuerst wird ein Model auf diesen geheimen Daten lokal trainiert. Anschließend wird Kontakt zu einem Klienten aufgenommen, welcher ebenfalls Daten sein Eigen nennt und das erstellte Model für seine Vorhersagen nutzen möchte. \n",
    "\n",
    "Das Model (ein neuronales Netzwerk) wird deshalb verschlüsselt. Der Klient verschlüsselt seine eigenen Daten. Daraufhin nutzen beide Beteiligte die aufgeteilten Anteile der Daten und des Models, um die verschlüsselten Daten zu klassifizieren. Schließlich wird das verschlüsselte Ergebnis zurück an den Klienten gesendet. Somit kann der Server nichts über die Daten oder deren Vorhersage schlussfolgern. \n",
    "\n",
    "Optimaler Weise müssten die Eingaben des `client` additiv aufgeteilt werden zwischen sich und dem `server`. Umgekehrt gilt dies genauso für das Model. Wegen der Übersichtlichkeit wird dieser Prozess hier auf die beiden Helfer `alice` und `bob` abgebildet. Wird angenommen Alice sei der Klient und Bob der Server, so ist das Beispiel equivalent.\n",
    "\n",
    "Diese Berechnungen sind sicher gegen einen \"ehrlichen-aber-neugierigen\" Angriff, welcher Standard in [vielen MPC Gerüsten](https://arxiv.org/pdf/1801.03239.pdf) ist.\n",
    "\n",
    "**Nun ist alles bereit und es kann begonnen werden!**\n",
    "\n",
    "Autoren:\n",
    "- Théo Ryffel - Twitter: [@theoryffel](https://twitter.com/theoryffel) · GitHub: [@LaRiffle](https://github.com/LaRiffle)\n",
    "\n",
    "Übersetzer:\n",
    "- Jan Moritz Behnken - Github: [@JMBehnken](https://github.com/JMBehnken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importe und Model Spezifikationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T09:57:40.194382Z",
     "start_time": "2020-04-04T09:57:39.771333Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch müssen Importe speziell für PySyft getätigt werden. Einige Helfer (genannt `client`, `bob` und `alice`) werden erstellt. Zuletzt wird der `crypto_provider` erzeugt, welcher die sichere Verschlüsselung ermöglicht ([genaueres in diesem Tutorial](https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/Part%2009%20-%20Intro%20to%20Encrypted%20Programs.ipynb))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:00:42.603753Z",
     "start_time": "2020-04-04T10:00:40.781235Z"
    }
   },
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch) \n",
    "client = sy.VirtualWorker(hook, id=\"client\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Einstellungen für das Training werden festgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:01:09.833869Z",
     "start_time": "2020-04-04T10:01:09.824799Z"
    }
   },
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 50\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.001\n",
    "        self.log_interval = 100\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Daten und versenden an die Helfer\n",
    "\n",
    "Zu den Grundannnahmen zählt, dass der Server Zugang zu Daten hat um ein Model zu trainieren. In diesem Fall ist es der MNIST Trainings-Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:02:38.171210Z",
     "start_time": "2020-04-04T10:02:38.042135Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weiterhin wird angenommen, dass der Klient eigene Daten besitzt und diese mit dem Model des Servers nutzen möchte. Dieser Klient verschlüsselt seine Daten indem er sie additiv auf die beiden Helfer `alice` und `bob` aufteilt. \n",
    "\n",
    "> SMPC verwendet Crypto-Protokolle, welche Integer voraussetzen. Dafür wird die PySyft Funktion `.fix_precision()` verwendet, welche Float-Tensoren dementsprechend anpasst. Beispielsweise wird 0.123 mit einer Genauigkeit von 2 auf die zweite Nachkommastelle gerundet und dann als Integer 12 gespeichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:08:08.030676Z",
     "start_time": "2020-04-04T10:08:05.470081Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True)\n",
    "\n",
    "private_test_loader = []\n",
    "for data, target in test_loader:\n",
    "    private_test_loader.append((\n",
    "        data.fix_precision().share(alice, bob, crypto_provider=crypto_provider),\n",
    "        target.fix_precision().share(alice, bob, crypto_provider=crypto_provider)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spezifikationen eines vorwärts gerichteten Neuronalen Netzwerkes\n",
    "\n",
    "Dies ist das Netzwerk vom Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:09:07.763461Z",
     "start_time": "2020-04-04T10:09:07.748353Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten des Trainings\n",
    "\n",
    "Das Training findet lokal auf der Maschine statt und wurde in reinem PyTorch umgesetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:09:51.316218Z",
     "start_time": "2020-04-04T10:09:51.299900Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(args, model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:12:19.619886Z",
     "start_time": "2020-04-04T10:09:52.098239Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, train_loader, optimizer, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:12:19.635243Z",
     "start_time": "2020-04-04T10:12:19.621233Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:12:21.401248Z",
     "start_time": "2020-04-04T10:12:19.636907Z"
    }
   },
   "outputs": [],
   "source": [
    "test(args, model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Model ist nun trainiert und bereit als Service angeboten zu werden!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sicheres Evaluieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Server verschlüsselt nun sein Model und versendet es an die Helfer. Weil das Model vertrauliche Informationen enthält (schließlich wurde Zeit und Geld in dessen Optimierung investiert), werden alle Matrizen des Models mittels additivem Aufteilen vor dem Versenden verschlüsselt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:12:43.226124Z",
     "start_time": "2020-04-04T10:12:43.172306Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fix_precision().share(alice, bob, crypto_provider=crypto_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Test Funktion berechnet eine verschlüsselte Evaluation. Die Gewichte des Models, die Eingabe-Daten, die Ausgabe-Daten und auch die Ziel-Variable sind dabei verschlüsselt!\n",
    "\n",
    "Trotzdem bleibt die Syntax sehr ähnlich verglichen mit dem Testen in reinem PyTorch.\n",
    "\n",
    "Schlussendlich wird einzig und allein die finale Bewertung des Models zurückgegeben und entschlüsselt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:16:04.540373Z",
     "start_time": "2020-04-04T10:16:04.523965Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()\n",
    "    n_correct_priv = 0\n",
    "    n_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader[:n_test_batches]:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1) \n",
    "            n_correct_priv += pred.eq(target.view_as(pred)).sum()\n",
    "            n_total += args.test_batch_size\n",
    "# This 'test' function performs the encrypted evaluation. The model weights, the data inputs, the prediction and the target used for scoring are all encrypted!\n",
    "\n",
    "# However as you can observe, the syntax is very similar to normal PyTorch testing! Nice!\n",
    "\n",
    "# The only thing we decrypt from the server side is the final score at the end of our 200 items batches to verify predictions were on average good.      \n",
    "            n_correct = n_correct_priv.copy().get().float_precision().long().item()\n",
    "    \n",
    "            print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                n_correct, n_total,\n",
    "                100. * n_correct / n_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T10:27:20.107154Z",
     "start_time": "2020-04-04T10:16:06.000737Z"
    }
   },
   "outputs": [],
   "source": [
    "test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! Es wurde vermittelt, wie Ende-zu-Ende sichere Vorhersagen umgesetzt werden können: die Gewichte des vom Server bereit gestellten Models sind weiterhin geheim und auch der Server konnte keinerlei Rückschlüsse über die Eingabe- und Ausgabe-Daten ziehen!\n",
    "\n",
    "Bezüglich der Performance dauert das Klassifizieren eines Bildes **weniger als 0.1 Sekunden**, ungefähr **33ms** auf dem genutzten Laptop (2,7 GHz Intel Core i7, 16GB RAM).  \n",
    "Jedoch kann eine schnelle Kommunikation angenommen werden, da alle Helfer auf dem lokalen Gerät angesiedelt sind. Die Performance wird schwanken, weil die einzelnen Helfer unterschiedlich schnell miteinander kommunizieren können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySyft auf GitHub einen Stern geben! \n",
    "\n",
    "Der einfachste Weg, unserer Community zu helfen, besteht darin, die GitHub-Repos mit Sternen auszuzeichnen! Dies hilft, das Bewusstsein für die coolen Tools zu schärfen, die wir bauen. \n",
    "\n",
    "- [Gib PySyft einen Stern](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Nutze unsere Tutorials auf GitHub!\n",
    "\n",
    "Wir haben hilfreiche Tutorials erstellt, um ein Verständnis für Federated und Privacy-Preserving Learning zu entwickeln und zu zeigen wie wir die einzelnen Bausteine weiter entwickeln.\n",
    "\n",
    "- [PySyft Tutorials ansehen](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)\n",
    "\n",
    "\n",
    "### Mach mit bei Slack! \n",
    "\n",
    "Der beste Weg, um über die neuesten Entwicklungen auf dem Laufenden zu bleiben, ist, sich unserer Community anzuschließen! Sie können dies tun, indem Sie das Formular unter [http://slack.openmined.org](http://slack.openmined.org) ausfüllen.\n",
    "\n",
    "### Treten Sie einem Code-Projekt bei! \n",
    "\n",
    "Der beste Weg, um zu unserer Community beizutragen, besteht darin, Entwickler zu werden! Sie können jederzeit zur PySyft GitHub Issues-Seite gehen und nach \"Projects\" filtern. Dies zeigt Ihnen alle Top-Level-Tickets und gibt einen Überblick darüber, an welchen Projekten Sie teilnehmen können! Wenn Sie nicht an einem Projekt teilnehmen möchten, aber ein wenig programmieren möchten, können Sie auch nach weiteren \"einmaligen\" Miniprojekten suchen, indem Sie nach GitHub-Problemen suchen, die als \"good first issue\" gekennzeichnet sind. \n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Spenden\n",
    "\n",
    "Wenn Sie keine Zeit haben, zu unserer Codebase beizutragen, aber dennoch Unterstützung leisten möchten, können Sie auch Unterstützer unseres Open Collective werden. Alle Spenden fließen in unser Webhosting und andere Community-Ausgaben wie Hackathons und Meetups! \n",
    "\n",
    " - [OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
