{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:05:36.883990Z",
     "start_time": "2020-04-04T13:05:36.876070Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 640\n",
    "n_test_items = 640"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil X - Sicheres Trainieren und Evaluieren mit MNIST\n",
    "\n",
    "Wenn es darum geht Machine Learning als Service (MLaaS) aufzubauen, müssen Unternehmen manchmal auf Daten eines anderen Partners zurückgreifen um ihr Model trainieren zu können. Im Gesundheits- oder Finanzbereich sind solche Daten jedoch extrem kritisch: während die Model Parameter einem Unternehmen gehören, sind die persönlichen Daten streng reguliert. \n",
    "\n",
    "In diesem Kontext stellt das vertrauliche Trainieren auf verschlüsselten Daten und Modelen eine mögliche Lösung dar. Dies garantiert, dass Unternehmen keinen direkten Zugang zu den Patientenakten bekommen und ebenso, dass Gesundheitsorganisationen keinen Einblick in das Model erhalten zu dem sie beitragen.  \n",
    "Es existieren unterschiedliche Verschlüsselungs Schemata, die für die Berechnungen mit verschlüsselten Daten verwendet werden können. Mit dazu gehören \"Secure Multi-Party Computation\" (SMPC), \"Homomorphic Encryption\" (FHE/SHE) und \"Functional Encryption\" (FE). Hier wird der Fokus auf \"Secure Multi-Party Computation\" mit seinem additiven Aufteilen gelegt. Aufgebaut ist alles auf den Crypto-Protokollen SecureNN und SPDZ.\n",
    "\n",
    "Die Ausgangslage für dieses Tutorial sieht wie folgt aus: es gibt einen Server, der sein Model auf den Daten von $n$ Helfern trainieren möchte. Der Server teilt sein Model dazu auf und versendet es an die Helfer. Die Helfer teilen gleichsam ihre Daten sicher auf und verteilen sie untereinander. In diesem Tutorial werden zwei Helfer (`alice` und `bob`) für den Aufbau verwendet. Nach dem Austausch besitzt jeder Helfer einen Anteil seiner eigenen Daten, einen Anteil der Daten des anderen Helfers und auch einen Anteil des Models. Das Training kann nun mit den jeweiligen Crypto-Protokollen gestartet werden. Sobald das Model trainiert ist, können alle Anteile an den Server zurückgesendet und entschlüsselt werden.  \n",
    "Das Vorgehen kann in der nachfolgenden Grafik nachvollzogen werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SMPC Illustration](https://github.com/OpenMined/PySyft/raw/11c85a121a1a136e354945686622ab3731246084/examples/tutorials/material/smpc_illustration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Beispiel für diesen Prozess wird angenommen, dass sowohl Alice wie auch Bob jeweils einen Teil des MNIST Datensatzes besitzen und darauf ein Model zum Klassifizieren der Ziffern trainiert werden soll. \n",
    "\n",
    "Autoren:\n",
    "- Théo Ryffel - Twitter: [@theoryffel](https://twitter.com/theoryffel) · GitHub: [@LaRiffle](https://github.com/LaRiffle)\n",
    "\n",
    "Übersetzer:\n",
    "- Jan Moritz Behnken - Github: [@JMBehnken](https://github.com/JMBehnken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Verschlüsseltes Trainings Beispiel mit MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importe und Trainings Konfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:22:49.571539Z",
     "start_time": "2020-04-04T13:22:49.141440Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Klasse beinhaltet alle Hyper-Parameter für das Training. Anzumerken ist, dass diese alle öffentlich bereitgestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:23:47.154910Z",
     "start_time": "2020-04-04T13:23:47.141271Z"
    }
   },
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.02\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1 # Log info at each batch\n",
    "        self.precision_fractional = 3\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "_ = torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es folgen die PySyft Importe. Eine Verbindung zu den beiden Helfern `alice` und `bob` wird hergestellt und auch ein `crypto_provider` für den sicheren Austausch wird erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:25:24.566927Z",
     "start_time": "2020-04-04T13:25:22.717217Z"
    }
   },
   "outputs": [],
   "source": [
    "import syft as sy  # import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # hook PyTorch to add extra functionalities like Federated and Encrypted Learning\n",
    "\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zugang zu den sicher geteilten Daten gewähren\n",
    "\n",
    "Hier wird eine nützliche Funktion verwendet, die folgendes Verhalten simuliert: es wird angenommen, dass der MNIST Datensatz zu teilen jeweils von den einzelnen Helfern gespeichert wird. Diese Helfer teilen ihre Daten in Batches auf und teilen diese wiederum sicher untereinander auf. Das finale Objekt ist ein Iterator über die sicher geteilten Batches und wird **vertraulicher Daten-Loader** genannt. Anzumerken ist, dass die lokale Machine dabei zu keiner Zeit Zugang zu den Rohdaten hat.\n",
    "\n",
    "Wie normalerweise auch, wird ein Trainings-Datensatz und ein Test-Datensatz bereit gestellt. In diesen sind die Eingabe-Daten und auch die Ziel-Variablen jeweils verschlüsselt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:31:43.156366Z",
     "start_time": "2020-04-04T13:31:32.613897Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def one_hot_of(index_tensor):\n",
    "        \"\"\"\n",
    "        Transform to one hot tensor\n",
    "        \n",
    "        Example:\n",
    "            [0, 3, 9]\n",
    "            =>\n",
    "            [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
    "            \n",
    "        \"\"\"\n",
    "        onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes for MNIST\n",
    "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "        return onehot_tensor\n",
    "        \n",
    "    def secret_share(tensor):\n",
    "        \"\"\"\n",
    "        Transform to fixed precision and secret share a tensor\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True, transform=transformation),\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(one_hot_of(target)))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True, transform=transformation),\n",
    "        batch_size=args.test_batch_size\n",
    "    )\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_test_items / args.test_batch_size\n",
    "    ]\n",
    "    \n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Spezifikationen\n",
    "\n",
    "Hier findet sich das zu verwendende Model. Es handelt sich um ein simples aber [erprobtes Model für MNIST](https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:32:43.089987Z",
     "start_time": "2020-04-04T13:32:43.074787Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainings und Test Funktionen\n",
    "\n",
    "Das Training läuft beinahe wie gewohnt ab. Der einzige Unterschied ist, dass der Loss nicht mit der negativen Log-Likelihood Methode (`F.nll_loss` in PyTorch) berechnet werden kann. Dies liegt daran, dass die Funktion schwer mit SMPC realisierbar ist. Stattdessen wird ein simpler \"Mean Squared Error\"-Loss verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:36:03.358333Z",
     "start_time": "2020-04-04T13:36:03.343229Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(args, model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get().float_precision()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Test Funktion wird nicht abgeändert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:36:15.827778Z",
     "start_time": "2020-04-04T13:36:15.814041Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(args, model, private_test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct.item(), len(private_test_loader)* args.test_batch_size,\n",
    "        100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starten des Trainings\n",
    "\n",
    "Hier eine kurze Zusammenfassung:  \n",
    "Zuerst wurden alle Model Parameter sicher auf alle Helfer verteilt. Danach wurden alle Parameter und Optimierer auf eine festgelegte Genauigkeit angepasst. Diese mussten nicht sicher aufgeteilt werden, weil sie in diesem Kontext als öffentlich angesehen werden konnten. Jedoch mussten auch sie mit `.fix_precision()` auf die konsistente Verarbeitung der Daten angepasst werden:  \n",
    "$W \\leftarrow W - \\alpha * \\Delta W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T13:46:47.300674Z",
     "start_time": "2020-04-04T13:36:31.017772Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "optimizer = optimizer.fix_precision() \n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, private_train_loader, optimizer, epoch)\n",
    "    test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit nur einem kleinen Teil des MNIST-Datensatzes konnte eine Genauigkeit von etwa 75% mit 100% verschlüsseltem Training erreicht werden!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Diskussion\n",
    "\n",
    "Die Mächtigkeit dieses verschlüsselten Trainings wird nun bleuchtet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Berechnungszeit\n",
    "\n",
    "Wie leicht zu erkennen war, dauerte das Training deutlich länger als auf lokalen Rohdaten. Im Speziellen dauert eine Iteration über einen Batch mit 64 Elementen etwa 3.2s, während er nur 13ms in reinem PyTorch benötigt. Während es vielleicht wie ein Hindernis erscheint, sollte bedacht werden, dass alles ferngesteuert und in verschlüsselter Weise ablief: keine einzige Datenzeile wurde offengelegt  \n",
    "Nachgemessen dauert das Berechnen eines einzigen Elements nur 50ms. Die wirkliche Herausforderun besteht darin zu analysieren wann verschlüsseltes Training von Nöten ist und wann einfache verschlüsselte Vorhersagen ausreichen. 50ms für eine Vorhersage zu benötigen ist in einem realen Szenario absolut akzeptabel!\n",
    "\n",
    "Ein Flaschenhals ist die zeitaufwändige Berechnung der Aktivierungsfunktion: die RELU-Aktivierung mit SMPC ist besonders langwierig, da sie intern Vergleiche und das SecureNN Protokoll verwendet. Wird die RELU-Funktion mit einer Quadratischen-Aktivierung ersetzt, so sinkt die Berechnungszeit von 3.2s auf 1.2s. Solch ein Vorgehen wird in mehreren Papern zu verschlüsselten Berechnungen wie z. B. CryptoNets vorgeschlagen.\n",
    "\n",
    "Der Kerngedanke sollte sein, nur das Nötige zu verschlüsseln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Backpropagation mit SMPC\n",
    "\n",
    "Wie Backpropagation und Gradienten-Updates rein mit Integern erreicht werden kann, mag verwirrend wirken. Dafür wurde ein neuer Syft Tensor names AutogradTensor entwickelt. In diesem Tutorial wurde er regelmäßig verwendet, auch wenn er nicht direkt in Erscheinung trat. Betrachten lässt er sich bei den Model Gewichten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T14:08:54.205402Z",
     "start_time": "2020-04-04T14:08:54.188293Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fc3.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinzufügen eines Datenpunktes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T14:09:07.696808Z",
     "start_time": "2020-04-04T14:09:07.686065Z"
    }
   },
   "outputs": [],
   "source": [
    "first_batch, input_data = 0, 0\n",
    "private_train_loader[first_batch][input_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der AutogradTensor ist deutlich zu erkennen. Er ist zwischen der Torch-Hülle und dem FixedPrecisionTensor angesiedelt. Dies deutet darauf hin, dass seine Werte ebenfalls im endlichen Integer Feld liegen. Ziel des AutogradTensors ist es den Berechnungs-Grafen der verschlüsselten Werte zu speichern. Nützlich wird er, sobald `.backward()` aufgerufen wird und der AutogradTensor dann die inkompatiblen PyTorch-Backward-Funktionen durch kompatible Funktionen für verschlüsselte Werte ersetzt. Beispielsweise bezüglich der Multiplikation, welche den Beaver-Triples-Trick anwendet, soll dieser Trick nicht selbst differenziert werden, sondern die einfache Multiplikation rein an sich: $\\partial_b (a \\cdot b) = a \\cdot \\partial b$.  \n",
    "Hier ist nachzulesen, wie die Gradienten berechnet werden:\n",
    "\n",
    "```python\n",
    "class MulBackward(GradFunc):\n",
    "    def __init__(self, self_, other):\n",
    "        super().__init__(self, self_, other)\n",
    "        self.self_ = self_\n",
    "        self.other = other\n",
    "\n",
    "    def gradient(self, grad):\n",
    "        grad_self_ = grad * self.other\n",
    "        grad_other = grad * self.self_ if type(self.self_) == type(self.other) else None\n",
    "        return (grad_self_, grad_other)\n",
    "```\n",
    "\n",
    "In `tensors/interpreters/gradients.py` kann nachgesehen werden, wie die anderen Gradienten implementiert sind.\n",
    "\n",
    "In Bezug auf den Berechnungs-Grafen bleibt eine Kopie lokal und der Server kann nicht nur den Forward-Anteil, sondern auch den Backward-Anteil koordinieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Sicherheitsgarantien\n",
    "\n",
    "Hier einige Hinweise auf die erreichte Sicherheit: Angriffe die hier betrachtet wurden, sind rein **ehrlich-aber-neugierig**. Das bedeutet, dass ein Angreifer nichts über die Daten innerhalb der Protokolle lernen kann, jedoch beim Abweichen von diesen Protokollen die ihm anvertrauten Anteile verändern und damit die Berechnung sabotieren kann. Die Sicherheit gegen bösartige Angriffe in solchen SMPC Szenarien bleibt weiterhin ein ungelöstes Problem.\n",
    "\n",
    "Zusätzlich gibt es weitere Bedrohungen aus der realen Welt, auch wenn die Secure Multi-Party Computation die Daten beim Training an sich schützen konnte. Beispielsweise können mit den aus Anfragen (im Kontext von MLaaS) resultierenden Vorhersagen möglicherweise Rückschlüsse auf die zugrundeliegenden Trainings-Datensätze gezogen werden. Im Speziellen gibt es keinen Schutz gegen Zugehörigkeits-Attacken, einer üblichen Attacke auf Machine Learning Services in der festgestellt werden soll, ob ein bestimmtes Element in dem Trainings-Datensatz enthalten war. Außerdem sind weitere Angriffe durch z. B. ungewolltes Auswendiglernen,  Model Inversion oder Model Extraktion weiterhin möglich. \n",
    "\n",
    "Eine effiziente Lösung gegen solche Bedrohungen kann \"Differential Privacy\" sein. Der Ansatz fügt sich nahtlos in die Secure Multi-Parti Computation ein und kann interesante Sicherheitsgarantien gewährleisten. Momentan wird an einigen Implementierungen gearbeitet und hoffentlich steht bald ein erstes Beispiel zur Verfügung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schlussfolgerung\n",
    "\n",
    "Gezeigt wurde, dass das Training eines Models mit SMPC vom Code her nicht kompliziert sein muss, auch wenn recht komplexe Strukturen verwendet wurden. Mit diesem Wissen im Hinterkopf können nun eigene Projekte analysiert werden, ob verschlüsselte Berechnungen benötigt werden für das Training oder die Evaluation. Wenn auch verschlüsselte Berechnungen im Allgemeinen langsamer sind, so können sie doch verwendet werden um die Gesamtzahl der Berechnungen zu reduzieren. \n",
    "\n",
    "Wenn Ihnen dies gefallen hat und Sie sich der Bewegung für mehr vertrauliche und dezentralisierte AI und AI Daten-Lieferkette anschließen möchten, können Sie das folgendermaßen tun.\n",
    "\n",
    "### PySyft auf GitHub einen Stern geben! \n",
    "\n",
    "Der einfachste Weg, unserer Community zu helfen, besteht darin, die GitHub-Repos mit Sternen auszuzeichnen! Dies hilft, das Bewusstsein für die coolen Tools zu schärfen, die wir bauen. \n",
    "\n",
    "- [Gib PySyft einen Stern](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Nutze unsere Tutorials auf GitHub!\n",
    "\n",
    "Wir haben hilfreiche Tutorials erstellt, um ein Verständnis für Federated und Privacy-Preserving Learning zu entwickeln und zu zeigen wie wir die einzelnen Bausteine weiter entwickeln.\n",
    "\n",
    "- [PySyft Tutorials ansehen](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)\n",
    "\n",
    "\n",
    "### Mach mit bei Slack! \n",
    "\n",
    "Der beste Weg, um über die neuesten Entwicklungen auf dem Laufenden zu bleiben, ist, sich unserer Community anzuschließen! Sie können dies tun, indem Sie das Formular unter [http://slack.openmined.org](http://slack.openmined.org) ausfüllen.\n",
    "\n",
    "### Treten Sie einem Code-Projekt bei! \n",
    "\n",
    "Der beste Weg, um zu unserer Community beizutragen, besteht darin, Entwickler zu werden! Sie können jederzeit zur PySyft GitHub Issues-Seite gehen und nach \"Projects\" filtern. Dies zeigt Ihnen alle Top-Level-Tickets und gibt einen Überblick darüber, an welchen Projekten Sie teilnehmen können! Wenn Sie nicht an einem Projekt teilnehmen möchten, aber ein wenig programmieren möchten, können Sie auch nach weiteren \"einmaligen\" Miniprojekten suchen, indem Sie nach GitHub-Problemen suchen, die als \"good first issue\" gekennzeichnet sind. \n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Spenden\n",
    "\n",
    "Wenn Sie keine Zeit haben, zu unserer Codebase beizutragen, aber dennoch Unterstützung leisten möchten, können Sie auch Unterstützer unseres Open Collective werden. Alle Spenden fließen in unser Webhosting und andere Community-Ausgaben wie Hackathons und Meetups! \n",
    "\n",
    " - [OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
